{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b822fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tiny shakespear was downloaded from: \n",
    "# https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58220e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of text\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "charachters = sorted(set(list(text))) #set contains only one of each character, can't have duplicates\n",
    "vocabular_size = len(charachters)\n",
    "print(''.join(charachters))\n",
    "print(vocabular_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea35495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the input text\n",
    "# This will be a character level -- if I want to make my own chatbots with my own data, \n",
    "# This part will be different, I think this is a very basic example. chat gpt uses a subword tokenizer\n",
    "# These function inverse the mapping between integers and string\n",
    "stoi = { ch:i for i, ch in enumerate(charachters) } # A dictionary of the charachter - integer mapping\n",
    "itos = { i:ch  for i, ch in enumerate(charachters)} # A dictionary of integer - charachter mapping\n",
    "encode = lambda s: [stoi[c] for c in s]  # text to tokens: s is input string, we make a list out of the lookups in stoi.\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # tokens to text: l is list of integers, we loop over it with i\n",
    "print(encode('hey, this is a test'))\n",
    "print(decode(encode('hey, this is a test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # A tensor is a multi dimensional array of data from a single data type. \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train the dataset with chunks at a time\n",
    "# We also split up the data in a training set, and a validation set\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # the length of chunks that will be fed to the model for training. in our case 8 charachters at a time.\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93385be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the principe of learning\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")\n",
    "print(range(block_size))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training batches\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will we process in parallel \n",
    "block_size = 8 # The maximum amount of context length for prediction\n",
    "# Basically this will determine the dimensions of the different py torch tensors.\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate random positions to grab chunk out of train / val data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # first block size characters, starting at I\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Offset of 1 from X. \n",
    "    return x, y \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d392a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(1337)\n",
    "# Constructor \n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabular_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocabular_size, vocabular_size) # tensor of shape vocab size x vocab size (65 x 65)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # Logits are basically the scores for the next character in the sequence.\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) (Batch == 4 , Time == 8 , Channel == 65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # evaluate loss / quality of predictions. quality of logits in retrospect to the targets\n",
    "    \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T : Batch, Time (block_size)) array of indices in the current context\n",
    "                #xb =\n",
    "                    #[\n",
    "                    #[57,  0, 58, 46, 39, 52,  1, 50],\n",
    "                    #[26, 19,  1, 20, 17, 26, 30, 37],\n",
    "                    #[43,  8,  0,  0, 31, 43, 41, 53],\n",
    "                    #[54, 54, 63,  1, 40, 56, 43, 31]\n",
    "                    #]\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "                #logits =\n",
    "                    #[\n",
    "                    #[ [65 scores], [65 scores], ... 8 times ... ],   # sequence 0\n",
    "                    #[ [65 scores], [65 scores], ... 8 times ... ],   # sequence 1\n",
    "                    #[ [65 scores], [65 scores], ... 8 times ... ],   # sequence 2\n",
    "                    #[ [65 scores], [65 scores], ... 8 times ... ]    # sequence 3\n",
    "                    #]\n",
    "            logits = logits[:, -1, :] # becomes (B, C) : Only use the predictions based on the last token in each row.\n",
    "                # logits\n",
    "                    # sequence 0 last token: 50\n",
    "                    # sequence 1 last token: 37\n",
    "                    # sequence 2 last token: 53\n",
    "                    # sequence 3 last token: 31\n",
    "            probs = F.softmax(logits, dim=-1) # Calculate probabilities\n",
    "                # probs\n",
    "                    # probs =\n",
    "                    #   [\n",
    "                    #   [p0, p1, p2, ... p64],   # sums to 1\n",
    "                    #   [p0, p1, p2, ... p64],\n",
    "                    #   [p0, p1, p2, ... p64],\n",
    "                    #   [p0, p1, p2, ... p64]\n",
    "                    #   ]\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) pick one tocken per row\n",
    "                # idx_next\n",
    "                    # idx_next =\n",
    "                    #   [\n",
    "                    #   [12],\n",
    "                    #   [ 0],\n",
    "                    #   [41],\n",
    "                    #   [63]\n",
    "                    #   ]\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "                    # before:\n",
    "                        # [57,  0, 58, 46, 39, 52,  1, 50]\n",
    "                    # after:\n",
    "                        # [57,  0, 58, 46, 39, 52,  1, 50, 12]   # example sampled token\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocabular_size)\n",
    "logits, loss = m(xb, yb) # xb = input, yb = targets\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())) # run the generation. it's gibberish now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())) # getting better structure\n",
    "# At this point, the character that is being predicted, is predicted based on one character. we now need to let it talk to the other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 \n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d856d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "    #wei = at time step T, you are allowed to look at positions 0 -> t\n",
    "    #    [\n",
    "    #    [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    #    [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    #    [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    #    [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    #    [1, 1, 1, 1, 1, 0, 0, 0],\n",
    "    #    [1, 1, 1, 1, 1, 1, 0, 0],\n",
    "    #    [1, 1, 1, 1, 1, 1, 1, 0],\n",
    "    #    [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    #    ]\n",
    "\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize each row to sum to 1.\n",
    "    #wei =\n",
    "    #    [\n",
    "    #    [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "    #    [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "    #    [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "    #    [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "    #    [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
    "    #    [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
    "    #    [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
    "    #    [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]\n",
    "    #    ]\n",
    "xbow2 = wei @ x # multiply by the logit, effecitvely getting the mean (weighted sums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eeea5fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
